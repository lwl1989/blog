CreateTime:2015-10-09 10:36:48.0

### 基本概念

> kNN算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 kNN方法在类别决策时，只与极少量的相邻样本有关。由于kNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，kNN方法较其他方法更为适合。


### 图例分析
![输入图片说明](http://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/220px-KnnClassification.svg.png "在这里输入图片标题")
如上图：
当我们选择的范围为实圆线圈的时候，有
k=3 red>bule  =>    green和red是同一类别；

当我们选择的范围为虚圆线圈的时候，有
k=5  blue>red  =>   green和blue是同一类别

当我们选择的范围全部的时候，有
k=11  blue>red  =>   green和blue是同一类别


### 说明
KNN方法虽然从原理上也依赖于极限定理，但在类别决策时，只与极少量的相邻样本有关。

> 极限定理：一般在同分布的情况下，样本值的和 在 总体数量趋于无穷时的极限分布，近似于正太分布。



### 适用场景和缺点

#### 适用场景

    对稀有事物进行分类，适合 多分类 的问题。

####缺点

    1.当样本数量不平衡时，不会影响结果。（由K值决定，范围内确定了）
    2.计算量大

#### 改进

    加权值
    =>(Weighted Abjusted KNN)

    结合上图:
    k=n,  red_result = sum(red)*权 ,blue_result = sum(blue)*权
    =>    
        当red_result>bule_result   green和red是同一类别
        反之 green和blue是同一类别；

### 计算

#### 过程
    1.预处理
    2.选定适合的数据结构
    3.设定K
    4.找出与数据最近的K个数据
        =》  List<结构> list = new List[k];

    5.计算当前元素与list中元素集合的距离
        =》
            L1=S1
            L2=S2
            ...
            Ln=Sn
            =》
                取S_min
    6.得出结果

[查阅大牛分析的简单过程](http://blog.csdn.net/jmydream/article/details/8644004)

    1）算距离：给定测试对象，计算它与训练集中的每个对象的距离

    2）找邻居：圈定距离最近的k个训练对象，作为测试对象的近邻

    3）做分类：根据这k个近邻归属的主要类别，来对测试对象分类

####类别的判断
    
    投票决定：少数服从多数，近邻中哪个类别的点最多就分为该类。

    加权投票法：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大（权重为距离平方的倒数）


### 常见问题
1、k值设定为多大？

k太小，分类结果易受噪声点影响；k太大，近邻中又可能包含太多的其它类别的点。（对距离加权，可以降低k值设定的影响）

k值通常是采用交叉检验来确定（以k=1为基准）

经验规则：k一般低于训练样本数的平方根

2、类别如何判定最合适？

投票法没有计算近邻的距离的远近，距离更近的近邻也许更应该决定最终的分类，所以加权投票法更恰当一些。

3、如何选择合适的距离衡量？

高维度对距离衡量的影响：众所周知当变量数越多，欧式距离的区分能力就越差。

变量值域对距离的影响：值域越大的变量常常会在距离计算中占据主导作用，因此应先对变量进行标准化。



4、训练样本是否要一视同仁？

在训练集中，有些样本可能是更值得依赖的。

可以给不同的样本施加不同的权重，加强依赖样本的权重，降低不可信赖样本的影响。



5、性能问题？

kNN是一种懒惰算法，平时不好好学习，要用时（对测试样本分类）时才临阵磨枪（临时去找k个近邻）。

懒惰的后果：构造模型很简单，但在对测试样本分类地的系统开销大，因为要扫描全部训练样本并计算距离。

已经有一些方法提高计算的效率，例如压缩训练样本量等。



6、能否大幅减少训练样本量，同时又保持分类精度？

浓缩技术(condensing)

编辑技术(editing)





